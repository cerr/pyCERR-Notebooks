{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerr/pyCERR-Notebooks/blob/main/autosegment_CT_Lung_OARs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNoydh-RsgcD"
      },
      "source": [
        "# The pyCERR Lung CT OAR Segmentation Model\n",
        "\n",
        "In this tutorial, we will demonstrate how to apply a pre-trained AI model to segment the OARs on a lung CT scan using pyCERR.  \n",
        "\n",
        "## Requirements\n",
        "* Python>=3.8\n",
        "* Applying this model requires access to a GPU.  \n",
        "  *On Colab* :  `Runtime > Change runtime type > Select GPU `\n",
        "\n",
        "## AI model\n",
        "* The segmentation model used here was trained and validated on CT scans used for RT planning. Its performance on diagnostic CTs is expected to be sub-optimal.\n",
        "* The trained model is packaged as a Conda environment archive containing  python libraries and other dependencies.\n",
        "\n",
        "  *On Colab* :  `Runtime > Change runtime type > Select GPU `\n",
        "\n",
        "## I/O\n",
        "* **Input**: DICOM-format planning lung CT scan(s).  \n",
        "  \n",
        "Input data should be organized as: one directory of DICOM images per patient.    \n",
        "  \n",
        "    \n",
        "    Input dir\n",
        "            |------Pat1  \n",
        "                      |------img1.dcm  \n",
        "                             img2.dcm  \n",
        "                             ....  \n",
        "                             ....  \n",
        "            |-----Pat2  \n",
        "                     |------img1.dcm  \n",
        "                            img2.dcm  \n",
        "                            ....  \n",
        "                            ....  \n",
        "\n",
        "\n",
        "* **Output**: DICOM RTStruct-format segmentations.\n",
        "\n",
        "\n",
        "## Running the model\n",
        "\n",
        "* The Conda archive is downloaded to a configurable `condaEnvPath`. By default `condaEnvPath = '/content/pretrainedModel/'`\n",
        "* The inference script is located at   \n",
        "`wrapperPath = os.path.join(condaEnvPath,'CT_LungOAR_incrMRRN/model_wrapper/run_inference_nii.py')`\n",
        "\n",
        "* Command to execute the model\n",
        "```python\n",
        "!python {wrapperPath} {input_nii_directory} {output_nii_directory}\n",
        "```\n",
        "* [***pyCERR***](https://github.com/cerr/pyCERR) is used for data pre- and post-processing, including converting DICOM to NIfTI format, required to run the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GVYj7k5e97G"
      },
      "source": [
        "## License\n",
        "\n",
        "```\n",
        "By downloading the software you are agreeing to the following terms and conditions as well as to the Terms of Use of CERR software.\n",
        "\n",
        "**THE SOFTWARE IS PROVIDED \"AS IS\" AND CERR DEVELOPMENT TEAM AND ITS COLLABORATORS DO NOT MAKE ANY WARRANTY, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, NOR DO THEY ASSUME ANY LIABILITY OR RESPONSIBILITY FOR THE USE OF THIS SOFTWARE.**\n",
        "\n",
        "This software is for research purposes only and has not been approved for clinical use.\n",
        "\n",
        "Software has not been reviewed or approved by the Food and Drug Administration, and is for non-clinical, IRB-approved Research Use Only. In no event shall data or images generated through the use of the Software be used in the provision of patient care.\n",
        "  \n",
        "YOU MAY NOT DISTRIBUTE COPIES of this software, or copies of software derived from this software, to others outside your organization without specific prior written permission from the CERR development team except where noted for specific software products.\n",
        "\n",
        "All Technology and technical data delivered under this Agreement are subject to US export control laws and may be subject to export or import regulations in other countries. You agree to comply strictly with all such laws and regulations and acknowledge that you have the responsibility to obtain\n",
        "such licenses to export, re-export, or import as may be required after delivery to you.\n",
        "\n",
        "**You may publish papers and books using results produced using software provided that you reference the following**:\n",
        "  \n",
        "  * ***AI model***: https://doi.org/10.1109/TMI.2018.2857800  \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35nEDWKTg5c1"
      },
      "source": [
        "# Downloads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP6FHznIg7Yd"
      },
      "source": [
        "\n",
        "## Download planning CTs (DICOM) from ***dataUrl*** to ***dataDownloadDir***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2cIR2Swg_84"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#workDir = r'/home/jupyter' #\n",
        "workDir = r'/content' #for Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install pyxnat\n",
        "! pip install \"pyCERR[napari] @ git+https://github.com/cerr/pyCERR.git@testing\""
      ],
      "metadata": {
        "id": "E0AfkOnWlaOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download planning CT DICOM data for processing"
      ],
      "metadata": {
        "id": "8j0u18h7litM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#location of input DICOM folders\n",
        "inputDicomPath = os.path.join(workDir,'input')\n",
        "os.makedirs(inputDicomPath, exist_ok=True)\n",
        "\n",
        "#location of output RTSTRUCT file\n",
        "outputDicomPath = os.path.join(workDir, 'output')\n",
        "os.makedirs(outputDicomPath, exist_ok=True)\n",
        "\n",
        "#temp session directory\n",
        "sessionPath = os.path.join(workDir, 'session')\n",
        "os.makedirs(sessionPath,exist_ok=True)\n"
      ],
      "metadata": {
        "id": "nrw7uxnvlpqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 1: Download data from XNAT source (`getXNATData`)"
      ],
      "metadata": {
        "id": "WdoMz5tLlnzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyxnat import Interface\n",
        "from glob import glob\n",
        "import urllib3\n",
        "import shutil\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "from pyxnat import Interface\n",
        "\n",
        "def getXNATData(xhost,user,scandict,downloadDir):\n",
        "  xnat = Interface(xhost, user, verify=False)\n",
        "  os.makedirs(downloadDir, exist_ok=True)\n",
        "  expdirlist = []\n",
        "  for scan_entry in scandict:\n",
        "    proj = scan_entry['proj']\n",
        "    subj = scan_entry['subj']\n",
        "    exp = scan_entry['exp']\n",
        "    scan_list = scan_entry['scan_list']\n",
        "    expdir = os.path.join(downloadDir,exp)\n",
        "    expdirlist.append(expdir)\n",
        "    os.makedirs(expdir, exist_ok = True)\n",
        "    xexp = xnat.select.project(proj).subject(subj).experiment(exp)\n",
        "    for scan in scan_list:\n",
        "      try:\n",
        "        xnat.select.project(proj).subject(subj).experiment(exp).scan(scan).resource('DICOM').get(downloadDir,extract=True)\n",
        "      except:\n",
        "        xnat.select.project(proj).subject(subj).experiment(exp).scan(scan).resource('secondary').get(downloadDir,extract=True)\n",
        "    for dcmfolder in ['DICOM','secondary']:\n",
        "      dcmlist = glob(os.path.join(downloadDir,dcmfolder,'*.dcm'))\n",
        "      print(dcmlist)\n",
        "      for dcm in dcmlist:\n",
        "        shutil.move(dcm, expdir)\n",
        "  for dcmfolder in ['DICOM','secondary']:\n",
        "    if os.path.exists(os.path.join(downloadDir,dcmfolder)):\n",
        "      os.rmdir(os.path.join(downloadDir,dcmfolder))\n",
        "    if os.path.exists(os.path.join(downloadDir,dcmfolder + '.zip')):\n",
        "      os.remove(os.path.join(downloadDir,dcmfolder + '.zip'))\n",
        "  xnat.disconnect()\n",
        "  return expdirlist"
      ],
      "metadata": {
        "id": "XY-VtqUKlh8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xhost = 'https://xnat.yoursite.org'\n",
        "user = 'usr'\n",
        "scandict = [{'proj':proj,'subj':subj,'exp':exp, 'scan_list':['1']}]\n",
        "\n",
        "folderList = getXNATData(xhost,user,scandict,inputDicomPath)"
      ],
      "metadata": {
        "id": "5RD0C22xl9Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option 2: Download data from other HTTP source to `inputDicomDir`"
      ],
      "metadata": {
        "id": "9oQ39Rl7l6E-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWaGs8LqhZTD"
      },
      "outputs": [],
      "source": [
        "dataUrl = 'http://path.to/data'\n",
        "dataDownloadDir = os.path.join(workDir,'tmp')\n",
        "os.makedirs(dataDownloadDir,exist_ok=True)\n",
        "\n",
        "! wget -O sampleData.gz -L {dataUrl}\n",
        "! tar xf sampleData.gz -C {dataDownloadDir}\n",
        "! rm sampleData.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install segmentation wrapper"
      ],
      "metadata": {
        "id": "Rd01-wUTmymF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation of segmentation wrapper, Python environment and network weights handled by CERR-developed `model_installer`"
      ],
      "metadata": {
        "id": "QaPeBe6hm5OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(workDir)\n",
        "!git clone https://github.com/cerr/model_installer.git\n",
        "os.chdir(os.path.join(workDir,'model_installer'))\n",
        "!./installer.sh -h"
      ],
      "metadata": {
        "id": "soip-h_CkzHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "modelOpt = '2' #CT_LungOAR_incrMRRN\n",
        "pythonOpt = 'C' #download and use pre-packaged Conda environment\n",
        "\n",
        "! source ./installer.sh -m {modelOpt} -d {workDir} -p {pythonOpt}"
      ],
      "metadata": {
        "id": "pMRe29URpZn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapperInstallDir = os.path.join(workDir, 'CT_LungOAR_incrMRRN')\n",
        "#!git clone https://github.com/cerr/CT_LungOAR_incrMRRN {scriptInstallDir}\n",
        "wrapperPath = os.path.join(wrapperInstallDir, 'model_wrapper','run_inference_nii.py')\n",
        "\n",
        "# Location of conda archive\n",
        "condaEnvDir = os.path.join(wrapperInstallDir, 'conda-pack')\n",
        "\n",
        "# Path to activation script for Conda environment\n",
        "condaActivateScript = os.path.join(condaEnvDir,'bin','activate')"
      ],
      "metadata": {
        "id": "ivspAad8nb5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfcIIiLQsgcM"
      },
      "source": [
        "# Function Definitions: Data pre- and post-processing using pyCERR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m955pAJsgcN"
      },
      "source": [
        "## `processInputData`: Identify the input scan and resize slices to 512x512 (pre-processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAXBqkTasgcN"
      },
      "outputs": [],
      "source": [
        "import cerr\n",
        "from cerr.dataclasses import structure\n",
        "from cerr.contour import rasterseg as rs\n",
        "from cerr.utils.aiPipeline import getScanNumFromIdentifier\n",
        "from cerr.utils import imageProc\n",
        "\n",
        "def processInputData(planC):\n",
        "\n",
        "  # Identify scan index in  planC\n",
        "  scanIdS = {\"imageType\": \"CT SCAN\"}\n",
        "  matchScanV = getScanNumFromIdentifier(scanIdS, planC,\n",
        "                                                    False)\n",
        "\n",
        "  # Extract scan\n",
        "  scanNum = matchScanV[0]\n",
        "  scan3M = planC.scan[scanNum].getScanArray()\n",
        "  mask3M = None\n",
        "\n",
        "  # Resize scan and import to planC\n",
        "  inputImgSizeV = np.shape(scan3M)\n",
        "  gridS = planC.scan[scanNum].getScanXYZVals()\n",
        "  outputImgSizeV = [512, 512, inputImgSizeV[2]]\n",
        "  method = 'padorcrop3d'\n",
        "  procScan3M, __, resizeGridS = imageProc.resizeScanAndMask(scan3M,\n",
        "                                mask3M, gridS, outputImgSizeV, method)\n",
        "\n",
        "  return procScan3M, resizeGridS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1KHjP1LsgcO"
      },
      "source": [
        "\n",
        "## `postProcAndImportSeg`: Import label maps and retain only the largest connected component to filter out false detections (post-processing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_GjRVWtsgcP"
      },
      "outputs": [],
      "source": [
        "#Import label map to CERR\n",
        "from cerr.utils import mask\n",
        "from cerr. dataclasses import structure\n",
        "\n",
        "def postProcAndImportSeg(outputDir,procScanNum,scanNum,planC):\n",
        "  niiGlob = glob(os.path.join(outputDir,'*.nii.gz'))\n",
        "\n",
        "  print('Importing ' + niiGlob[0]+'...')\n",
        "  numStrOrig = len(planC.structure)\n",
        "  planC = pc.load_nii_structure(niiGlob[0], procScanNum, planC, \\\n",
        "                              labels_dict = strToLabelMap)\n",
        "  cpyStrNumV = np.arange(numStrOrig,len(planC.structure))\n",
        "  numComponents = 1\n",
        "  for label in range(numLabel):\n",
        "    # Copy to original scan\n",
        "    planC = structure.copyToScan(cpyStrNumV[label], scanNum, planC)\n",
        "    origStr = len(planC.structure)-1\n",
        "    strName =  strToLabelMap[label+1]\n",
        "    # Post-process and replace input structure in planC\n",
        "    procMask3M = structure.getLargestConnComps(origStr, numComponents,\n",
        "                                                planC, saveFlag=True,\n",
        "                                                replaceFlag=False,\n",
        "                                                procSructName=strName)\n",
        "  return planC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj7t-J4isgcQ"
      },
      "source": [
        "# Run segmentation: Generate OARs for all the CT scan folders located at `inputDicomPath`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtPOzqWQsgcP"
      },
      "outputs": [],
      "source": [
        "# Map output labels to structure names\n",
        "\n",
        "strToLabelMap = {1:\"Lung_Left\", 2:\"Lung_Right\", 3:\"Heart\", 4:\"Esophagus\", \\\n",
        "                 5:\"Cord\", 6:\"PBT\"}\n",
        "numLabel = len(strToLabelMap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCuUi5D6sgcS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import cerr\n",
        "from cerr import plan_container as pc\n",
        "from cerr.dataclasses import scan as cerrScn\n",
        "from cerr.utils.aiPipeline import createSessionDir\n",
        "from cerr.dcm_export import rtstruct_iod\n",
        "\n",
        "# Loop over pyCERR files\n",
        "folderList = glob(os.path.join(inputDicomPath,'*'))\n",
        "modality = 'CT'\n",
        "scanNum = 0\n",
        "\n",
        "for dcmDir in folderList:\n",
        "    fname = os.path.basename(dcmDir)\n",
        "    # Create session dir to store temporary data\n",
        "    modInputPath, modOutputPath = createSessionDir(sessionPath, dcmDir)\n",
        "\n",
        "\n",
        "    # Import DICOM scan to planC\n",
        "    planC = pc.load_dcm_dir(dcmDir)\n",
        "    numExistingStructs = len(planC.structure)\n",
        "\n",
        "    # Pre-process data\n",
        "    procScan3M, resizeGridS = processInputData(planC)\n",
        "    planC = pc.import_scan_array(procScan3M, resizeGridS[0], \\\n",
        "            resizeGridS[1], resizeGridS[2], modality, scanNum, planC)\n",
        "    procScanNum = len(planC.scan) - 1\n",
        "\n",
        "    # Export inputs to NIfTI\n",
        "    scanFilename = os.path.join(modInputPath,\n",
        "                                f\"{fname}_scan_3D.nii.gz\")\n",
        "    planC.scan[procScanNum].save_nii(scanFilename)\n",
        "\n",
        "    # Apply model\n",
        "    subprocess.run(f\"source {condaActivateScript} && python {wrapperPath} \\\n",
        "                  {modInputPath} {modOutputPath}\", \\\n",
        "                  capture_output=False,shell=True,executable=\"/bin/bash\")\n",
        "\n",
        "    # Import results to planC\n",
        "    planC = postProcAndImportSeg(modOutputPath,procScanNum,scanNum,planC)\n",
        "    newNumStructs = len(planC.structure)\n",
        "\n",
        "    # Export segmentations to DICOM\n",
        "    structFileName = fname +'_AI_seg_RTSTRUCT.dcm'\n",
        "    structFilePath = os.path.join(outputDicomPath,structFileName)\n",
        "    structNumV = np.arange(numExistingStructs,newNumStructs)\n",
        "    indOrigV = np.array([cerrScn.getScanNumFromUID(planC.structure[structNum].assocScanUID,\\\n",
        "                        planC) for structNum in structNumV], dtype=int)\n",
        "    origIndsToExportV = structNumV[indOrigV == scanNum]\n",
        "    seriesDescription = \"AI Generated\"\n",
        "    exportOpts = {'seriesDescription': seriesDescription}\n",
        "    rtstruct_iod.create(structsToExportV,structFilePath,planC,exportOpts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kugws3KBl4MK"
      },
      "source": [
        "## **Optional**: Uncomment the following to download the output segmentations to your workspace bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nkTBkijl23J"
      },
      "outputs": [],
      "source": [
        "# workspaceBucket = os.environ['WORKSPACE_BUCKET']\n",
        "# !gcloud storage cp -r {outputDicomPath} {workspaceBucket}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLzFp-l_sgcT"
      },
      "source": [
        "# Display results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8FTwNqhsgcT"
      },
      "source": [
        "## Overlay AI segmentations on scan for visualization using ***Matplotlib***\n",
        "\n",
        "Note: This example displays the last segmented dataset by default.  \n",
        "Load the appropriate pyCERR archive to `planC` to view results for desired dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWQD1KeVsgcT"
      },
      "outputs": [],
      "source": [
        "from cerr.viewer import showMplNb\n",
        "\n",
        "showMplNb(scanNum, origIndsToExportV, planC,\\\n",
        "          windowCenter=-400, windowWidth=2000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUqd_h7G1Jbn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}